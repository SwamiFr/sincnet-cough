{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1ry4c-NskyYIkBWRJ2uuMVmsOlFzfN8qy",
      "authorship_tag": "ABX9TyO7rNpIQ/kq6JZJJGJG5y9K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SwamiFr/sincnet-cough/blob/main/Sincnet_Deneme_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drive daki bilgileri local dataya aktarma\n"
      ],
      "metadata": {
        "id": "dUDLQlHu4XqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerekli kütüphaneleri yükle\n",
        "!pip install torchcodec\n",
        "!pip install soundfile\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "\n",
        "print(\"Kütüphaneler yüklendi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGyZeqw59U0F",
        "outputId": "f6bcadcc-7be7-419e-d8c9-fdc45f4a5a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchcodec\n",
            "  Downloading torchcodec-0.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Downloading torchcodec-0.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchcodec\n",
            "Successfully installed torchcodec-0.9.1\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (2.23)\n",
            "Kütüphaneler yüklendi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. Drive'ı bağla\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Kaynak Yolları\n",
        "# Sağlıklı verilerin olduğu klasör\n",
        "healthy_source = '/content/drive/MyDrive/Colab Notebooks/İşaretlenenler/healthy/other'\n",
        "\n",
        "# Astım verilerin olduğu klasör\n",
        "asthma_source = '/content/drive/MyDrive/Colab Notebooks/İşaretlenenler/asthma'\n",
        "\n",
        "# Hedef klasörler (Colab Local - Hızlı Disk)\n",
        "base_target = '/content/dataset'\n",
        "healthy_target_dir = os.path.join(base_target, 'healthy')\n",
        "asthma_target_dir = os.path.join(base_target, 'asthma')\n",
        "\n",
        "# Hedef klasörleri temizle ve yeniden oluştur\n",
        "if os.path.exists(base_target):\n",
        "    shutil.rmtree(base_target)\n",
        "os.makedirs(healthy_target_dir)\n",
        "os.makedirs(asthma_target_dir)\n",
        "\n",
        "print(\"Dosyalar taranıyor ve kopyalanıyor...\")\n",
        "\n",
        "# Kopyalama Fonksiyonu\n",
        "def copy_files(source_folder, target_folder, label_name):\n",
        "    count = 0\n",
        "    # os.walk alt klasörlerin hepsine girer\n",
        "    for root, dirs, files in os.walk(source_folder):\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.wav'):\n",
        "                src_path = os.path.join(root, file)\n",
        "                dst_path = os.path.join(target_folder, file)\n",
        "\n",
        "\n",
        "\n",
        "                shutil.copy(src_path, dst_path)\n",
        "                count += 1\n",
        "    print(f\"-> '{label_name}' için kopyalanan dosya sayısı: {count}\")\n",
        "    return count\n",
        "\n",
        "# İşlemi Başlat\n",
        "h_count = copy_files(healthy_source, healthy_target_dir, \"Sağlıklı (healthy)\")\n",
        "a_count = copy_files(asthma_source, asthma_target_dir, \"Astım (asthma)\")\n",
        "\n",
        "print(f\"\\nİşlem Tamamlandı!\")\n",
        "print(f\"Toplam Ses Dosyası: {h_count + a_count}\")\n",
        "\n",
        "# Hata kontrolü:\n",
        "if h_count == 0:\n",
        "    print(f\"UYARI: '{healthy_source}' konumunda hiç .wav dosyası bulunamadı! Drive yolunu kontrol et.\")\n",
        "if a_count == 0:\n",
        "    print(f\"UYARI: '{asthma_source}' konumunda hiç .wav dosyası bulunamadı! Drive yolunu kontrol et.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26v1XJN24XzK",
        "outputId": "4d722200-d70e-4720-dd3c-df41004108bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dosyalar taranıyor ve kopyalanıyor...\n",
            "-> 'Sağlıklı (healthy)' için kopyalanan dosya sayısı: 721\n",
            "-> 'Astım (asthma)' için kopyalanan dosya sayısı: 346\n",
            "\n",
            "İşlem Tamamlandı!\n",
            "Toplam Ses Dosyası: 1067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veri Yükleyici (Dataset) ve Eğitim/Test Ayrımı"
      ],
      "metadata": {
        "id": "7oo3UQ6F738v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ADIM 2 (AUGMENTATION EKLENDİ) ---\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "DURATION = 3\n",
        "INPUT_LENGTH = SAMPLE_RATE * DURATION\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Yollar\n",
        "healthy_dir = '/content/dataset/healthy'\n",
        "asthma_dir = '/content/dataset/asthma'\n",
        "\n",
        "class CoughDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, target_length, train_mode=False):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.target_length = target_length\n",
        "        self.train_mode = train_mode # Eğitim modunda mı?\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        try:\n",
        "            waveform, sr = torchaudio.load(path, backend=\"soundfile\")\n",
        "        except:\n",
        "            waveform, sr = torchaudio.load(path)\n",
        "\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "        if sr != SAMPLE_RATE:\n",
        "            resampler = torchaudio.transforms.Resample(sr, SAMPLE_RATE)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        # Uzunluk sabitleme\n",
        "        if waveform.shape[1] > self.target_length:\n",
        "            waveform = waveform[:, :self.target_length]\n",
        "        elif waveform.shape[1] < self.target_length:\n",
        "            padding = self.target_length - waveform.shape[1]\n",
        "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
        "\n",
        "        # --- AUGMENTATION (Sadece Eğitimde) ---\n",
        "        # Modele sürekli aynı temiz sesi vermek yerine biraz gürültülü veriyoruz\n",
        "        if self.train_mode:\n",
        "            # 1. Gaussian Noise (Cızırtı ekle)\n",
        "            if torch.rand(1).item() < 0.5: # %50 ihtimalle\n",
        "                noise = torch.randn_like(waveform) * 0.005\n",
        "                waveform = waveform + noise\n",
        "\n",
        "            # 2. Amplitude Shift (Ses şiddetini rastgele değiştir)\n",
        "            vol_factor = np.random.uniform(0.8, 1.2)\n",
        "            waveform = waveform * vol_factor\n",
        "\n",
        "        return waveform, label\n",
        "\n",
        "# Dosya işlemleri aynı\n",
        "healthy_files = glob.glob(os.path.join(healthy_dir, \"*.wav\"))\n",
        "asthma_files = glob.glob(os.path.join(asthma_dir, \"*.wav\"))\n",
        "\n",
        "X = healthy_files + asthma_files\n",
        "y = [0] * len(healthy_files) + [1] * len(asthma_files)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "# Train dataset'e 'train_mode=True' dedik!\n",
        "train_dataset = CoughDataset(X_train, y_train, INPUT_LENGTH, train_mode=True)\n",
        "test_dataset = CoughDataset(X_test, y_test, INPUT_LENGTH, train_mode=False) # Test verisi saf kalmalı\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"Veri seti 'Data Augmentation' ile güçlendirildi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUepFP3S78Ak",
        "outputId": "5931cb1f-6d5d-4ff6-f58f-b0406e590a7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Veri seti 'Data Augmentation' ile güçlendirildi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SincNet Mimarisi"
      ],
      "metadata": {
        "id": "bO5R0aAQ8Bdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class SincConv_fast(nn.Module):\n",
        "    def __init__(self, out_channels, kernel_size, sample_rate=16000, in_channels=1,\n",
        "                 min_low_hz=50, min_band_hz=50):\n",
        "        super(SincConv_fast, self).__init__()\n",
        "\n",
        "        if in_channels != 1:\n",
        "            raise ValueError(\"SincConv sadece tek kanal giriş destekler.\")\n",
        "\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        # Simetri için kernel boyutu tek sayı olmalı\n",
        "        if kernel_size % 2 == 0:\n",
        "            self.kernel_size = self.kernel_size + 1\n",
        "\n",
        "        self.sample_rate = sample_rate\n",
        "        self.min_low_hz = min_low_hz\n",
        "        self.min_band_hz = min_band_hz\n",
        "\n",
        "        # Mel-scale başlatma\n",
        "        low_hz = 30\n",
        "        high_hz = sample_rate / 2 - (min_low_hz + min_band_hz)\n",
        "\n",
        "        mel = np.linspace(self.to_mel(low_hz), self.to_mel(high_hz), self.out_channels + 1)\n",
        "        hz = self.to_hz(mel)\n",
        "\n",
        "        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n",
        "        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n",
        "\n",
        "\n",
        "        # n_lin 0'dan başlarsa (sin(0)/0) hatası verir ve Loss NaN olur.\n",
        "        # Bu yüzden 1'den başlatıyoruz. Merkez nokta (0) zaten aşağıda 'band_pass_center' ile hesaplanıyor.\n",
        "\n",
        "        # Hamming penceresi\n",
        "        n_lin = torch.linspace(1, (self.kernel_size / 2), int((self.kernel_size / 2))) # 0 yerine 1'den başlattık\n",
        "        window_ = 0.54 - 0.46 * torch.cos(2 * np.pi * n_lin / self.kernel_size)\n",
        "\n",
        "        # Zaman ekseni (n_)\n",
        "        n_ = 2 * np.pi * n_lin / self.sample_rate\n",
        "        n_ = n_.view(1, -1)\n",
        "\n",
        "        # Buffer kayıt\n",
        "        self.register_buffer('window_', window_)\n",
        "        self.register_buffer('n_', n_)\n",
        "\n",
        "    def to_mel(self, hz):\n",
        "        return 2595 * np.log10(1 + hz / 700)\n",
        "\n",
        "    def to_hz(self, mel):\n",
        "        return 700 * (10 ** (mel / 2595) - 1)\n",
        "\n",
        "    def forward(self, waveforms):\n",
        "        low = self.min_low_hz + torch.abs(self.low_hz_)\n",
        "        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_), self.min_low_hz, self.sample_rate / 2)\n",
        "        band = (high - low)[:, 0]\n",
        "\n",
        "        f_times_t_low = torch.matmul(low, self.n_)\n",
        "        f_times_t_high = torch.matmul(high, self.n_)\n",
        "\n",
        "        # Sinc fonksiyonu\n",
        "        band_pass_left = ((torch.sin(f_times_t_high) - torch.sin(f_times_t_low)) / (self.n_ / 2)) * self.window_\n",
        "        band_pass_center = 2 * band.view(-1, 1)\n",
        "        band_pass_right = torch.flip(band_pass_left, dims=[1])\n",
        "\n",
        "        filters = torch.cat([band_pass_left, band_pass_center, band_pass_right], dim=1)\n",
        "        filters = filters.view(self.out_channels, 1, self.kernel_size)\n",
        "\n",
        "        return F.conv1d(waveforms, filters, stride=1, padding=self.kernel_size // 2)\n",
        "\n",
        "# --- ADIM 3 (DROPOUT EKLENDİ) ---\n",
        "\n",
        "class CoughSincNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(CoughSincNet, self).__init__()\n",
        "\n",
        "        self.sinc_conv = SincConv_fast(out_channels=80, kernel_size=251, sample_rate=16000)\n",
        "        self.layer_norm = nn.LayerNorm([80, INPUT_LENGTH])\n",
        "\n",
        "        self.conv2 = nn.Conv1d(80, 60, kernel_size=5)\n",
        "        self.bn2 = nn.BatchNorm1d(60)\n",
        "        self.pool2 = nn.MaxPool1d(3)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(60, 60, kernel_size=5)\n",
        "        self.bn3 = nn.BatchNorm1d(60)\n",
        "        self.pool3 = nn.MaxPool1d(3)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        with torch.no_grad():\n",
        "             dummy = torch.zeros(1, 1, INPUT_LENGTH)\n",
        "             x = self.sinc_conv(dummy)\n",
        "             x = self.layer_norm(x)\n",
        "             x = self.pool2(F.leaky_relu(self.bn2(self.conv2(x))))\n",
        "             x = self.pool3(F.leaky_relu(self.bn3(self.conv3(x))))\n",
        "             flatten_dim = x.shape[1] * x.shape[2]\n",
        "\n",
        "        # --- DROPOUT EKLENDİ ---\n",
        "        self.dropout = nn.Dropout(0.5) # Nöronların %50'sini kapat\n",
        "\n",
        "        self.fc1 = nn.Linear(flatten_dim, 256)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.layer_norm(self.sinc_conv(x)))\n",
        "        x = self.pool2(F.leaky_relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool3(F.leaky_relu(self.bn3(self.conv3(x))))\n",
        "\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        # Dropout'u buraya ekliyoruz\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = F.leaky_relu(self.bn_fc1(self.fc1(x)))\n",
        "\n",
        "        # Çıkıştan önce de biraz dropout koyabiliriz (opsiyonel)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CoughSincNet(num_classes=2).to(device)\n",
        "print(f\"Model Dropout katmanlarıyla güçlendirildi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Doqqiuk38DYS",
        "outputId": "87ca52ae-720a-4802-9466-905bc694bc38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Dropout katmanlarıyla güçlendirildi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eğitim (Training) Döngüsü"
      ],
      "metadata": {
        "id": "kMTp4Y708vpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ADIM 4: TAM EĞİTİM KODU (Early Stopping Dahil) ---\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Öncelikle 'run_epoch' fonksiyonunu tanımlayalım\n",
        "def run_epoch(loader, is_train=True):\n",
        "    model.train() if is_train else model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # İlerleme çubuğu (Progress Bar)\n",
        "    desc = \"Eğitim\" if is_train else \"Test\"\n",
        "    loop = tqdm(loader, desc=desc, leave=False, file=sys.stdout)\n",
        "\n",
        "    with torch.set_grad_enabled(is_train):\n",
        "        for inputs, labels in loop:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            if is_train:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # NaN koruması\n",
        "                if torch.isnan(loss):\n",
        "                    continue\n",
        "\n",
        "                loss.backward()\n",
        "                # Gradient Clipping\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "            else:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = total_loss / len(loader) if len(loader) > 0 else 0\n",
        "    return avg_loss, 100 * correct / total\n",
        "\n",
        "# --- Ayarlar ve Döngü ---\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "EPOCHS = 30\n",
        "\n",
        "# Early Stopping Değişkenleri\n",
        "patience = 5\n",
        "best_loss = float('inf')\n",
        "counter = 0\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "print(f\"--- GELİŞMİŞ EĞİTİM BAŞLIYOR (Cihaz: {device}) ---\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = run_epoch(train_loader, is_train=True)\n",
        "    test_loss, test_acc = run_epoch(test_loader, is_train=False)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} -> \"\n",
        "          f\"Train Loss: {train_loss:.4f} | \"\n",
        "          f\"Test Loss: {test_loss:.4f} (Acc: %{test_acc:.2f})\")\n",
        "\n",
        "    # Early Stopping Kontrolü\n",
        "    if test_loss < best_loss:\n",
        "        best_loss = test_loss\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        counter = 0\n",
        "        # En iyi modeli diske de kaydedelim\n",
        "        torch.save(model.state_dict(), 'en_iyi_model.pth')\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"   -> İyileşme yok. Sayaç: {counter}/{patience}\")\n",
        "        if counter >= patience:\n",
        "            print(\"--- ERKEN DURDURMA (EARLY STOPPING) ---\")\n",
        "            print(\"Model ezberlemeye (overfitting) başladı, eğitim durduruluyor.\")\n",
        "            break\n",
        "\n",
        "# En iyi ağırlıkları geri yükle\n",
        "model.load_state_dict(best_model_wts)\n",
        "print(f\"Eğitim bitti. En iyi model yüklendi. (Test Loss: {best_loss:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfqOBJrn8zcJ",
        "outputId": "9f8d3bb3-3152-4e29-995f-3ad28e324871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- GELİŞMİŞ EĞİTİM BAŞLIYOR (Cihaz: cuda) ---\n",
            "Eğitim:   0%|          | 0/54 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchaudio/__init__.py:86: UserWarning: The 'backend' parameter is not used by TorchCodec AudioDecoder.\n",
            "  return load_with_torchcodec(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30 -> Train Loss: 0.6751 | Test Loss: 0.5244 (Acc: %78.04)\n",
            "Epoch 2/30 -> Train Loss: 0.4037 | Test Loss: 0.5085 (Acc: %78.97)\n",
            "Epoch 3/30 -> Train Loss: 0.2789 | Test Loss: 0.4376 (Acc: %82.71)\n",
            "Epoch 4/30 -> Train Loss: 0.2002 | Test Loss: 0.4366 (Acc: %82.71)\n",
            "Epoch 5/30 -> Train Loss: 0.1529 | Test Loss: 0.4839 (Acc: %82.71)\n",
            "   -> İyileşme yok. Sayaç: 1/5\n",
            "Epoch 6/30 -> Train Loss: 0.1074 | Test Loss: 0.4919 (Acc: %81.78)\n",
            "   -> İyileşme yok. Sayaç: 2/5\n",
            "Epoch 7/30 -> Train Loss: 0.0727 | Test Loss: 0.5452 (Acc: %82.24)\n",
            "   -> İyileşme yok. Sayaç: 3/5\n",
            "Epoch 8/30 -> Train Loss: 0.0754 | Test Loss: 0.5067 (Acc: %82.71)\n",
            "   -> İyileşme yok. Sayaç: 4/5\n",
            "Epoch 9/30 -> Train Loss: 0.0608 | Test Loss: 0.5075 (Acc: %80.37)\n",
            "   -> İyileşme yok. Sayaç: 5/5\n",
            "--- ERKEN DURDURMA (EARLY STOPPING) ---\n",
            "Model ezberlemeye (overfitting) başladı, eğitim durduruluyor.\n",
            "Eğitim bitti. En iyi model yüklendi. (Test Loss: 0.4366)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SEgmente ve tahmin\n"
      ],
      "metadata": {
        "id": "MSTlKYJjyvUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Ayarlar\n",
        "TARGET_SAMPLE_RATE = 16000\n",
        "INPUT_LENGTH = 48000 # 3 saniye * 16000\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def process_and_predict(segment, sr, model):\n",
        "    \"\"\"Tek bir ses parçasını modele sokar ve sonucu döndürür\"\"\"\n",
        "    # 1. Tensor'a çevir\n",
        "    tensor = torch.from_numpy(segment).float()\n",
        "\n",
        "    # 2. Boyutlandırma (Padding veya Cutting)\n",
        "    if tensor.shape[0] > INPUT_LENGTH:\n",
        "        # Uzunsa ortadan kes\n",
        "        center = tensor.shape[0] // 2\n",
        "        start = center - (INPUT_LENGTH // 2)\n",
        "        tensor = tensor[start : start + INPUT_LENGTH]\n",
        "    elif tensor.shape[0] < INPUT_LENGTH:\n",
        "        # Kısaysa sonuna sıfır ekle (Padding)\n",
        "        padding = INPUT_LENGTH - tensor.shape[0]\n",
        "        tensor = F.pad(tensor, (0, padding))\n",
        "\n",
        "    # 3. Batch boyutu ve Kanal ekle [1, 1, 48000]\n",
        "    input_tensor = tensor.unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    # 4. Tahmin\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        probs = F.softmax(output, dim=1)\n",
        "        conf, pred = torch.max(probs, 1)\n",
        "\n",
        "    return pred.item(), conf.item()\n",
        "\n",
        "def analyze_audio_file(file_path):\n",
        "    print(f\"ANALİZ BAŞLIYOR: {file_path}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(\"HATA: Dosya bulunamadı! Yolu kontrol et.\")\n",
        "        return\n",
        "\n",
        "    # 1. Dosyayı Yükle (Librosa ile)\n",
        "    try:\n",
        "        y, sr = librosa.load(file_path, sr=TARGET_SAMPLE_RATE)\n",
        "    except Exception as e:\n",
        "        print(f\"Dosya okuma hatası: {e}\")\n",
        "        return\n",
        "\n",
        "    # 2. Sessizlikleri At (Segmentation)\n",
        "    # top_db=20: Gürültü eşiği (sessiz yerleri atar)\n",
        "    intervals = librosa.effects.split(y, top_db=20)\n",
        "\n",
        "    votes = {\"healthy\": 0, \"asthma\": 0}\n",
        "    results_log = []\n",
        "\n",
        "    print(f\"Tespit edilen ses parçası (öksürük adayı) sayısı: {len(intervals)}\\n\")\n",
        "\n",
        "    for i, (start, end) in enumerate(intervals):\n",
        "        segment = y[start:end]\n",
        "\n",
        "        # Çok kısa sesleri (0.2 saniyeden az) yoksay (tıkırtı vb.)\n",
        "        if len(segment) < (0.2 * TARGET_SAMPLE_RATE):\n",
        "            continue\n",
        "\n",
        "        # Tahmin Yap\n",
        "        label_idx, confidence = process_and_predict(segment, sr, model)\n",
        "\n",
        "        # Etiketleme (0: Sağlıklı, 1: Astım)\n",
        "        label_str = \"SAĞLIKLI\" if label_idx == 0 else \"ASTIM\"\n",
        "        color = \"\\033[92m\" if label_idx == 0 else \"\\033[91m\" # Yeşil veya Kırmızı yazı\n",
        "        reset = \"\\033[0m\"\n",
        "\n",
        "        print(f\"Parça {i+1}: {color}{label_str}{reset} (Güven: %{confidence*100:.2f}) - Süre: {len(segment)/sr:.2f}sn\")\n",
        "\n",
        "        # Oylama (Sadece güveni %60'tan yüksek olanları say)\n",
        "        if confidence > 0.60:\n",
        "            if label_idx == 0:\n",
        "                votes[\"healthy\"] += 1\n",
        "            else:\n",
        "                votes[\"asthma\"] += 1\n",
        "\n",
        "        results_log.append(label_idx)\n",
        "\n",
        "    # 3. Genel Sonuç\n",
        "    print(\"-\" * 50)\n",
        "    print(\"GENEL RAPOR:\")\n",
        "\n",
        "    total_valid_votes = votes[\"healthy\"] + votes[\"asthma\"]\n",
        "\n",
        "    if total_valid_votes == 0:\n",
        "        print(\"Yeterli netlikte öksürük tespit edilemedi veya dosya boş.\")\n",
        "    else:\n",
        "        asthma_ratio = (votes[\"asthma\"] / total_valid_votes) * 100\n",
        "\n",
        "        print(f\"Astım Oyu: {votes['asthma']}\")\n",
        "        print(f\"Sağlıklı Oyu: {votes['healthy']}\")\n",
        "\n",
        "        if votes[\"asthma\"] > votes[\"healthy\"]:\n",
        "            print(f\"\\nSONUÇ: \\033[91m⚠️ BU SES ASTIM BELİRTİLERİ GÖSTERİYOR ⚠️\\033[0m\")\n",
        "        elif votes[\"healthy\"] > votes[\"asthma\"]:\n",
        "            print(f\"\\nSONUÇ: \\033[92m✅ BU SES SAĞLIKLI GÖRÜNÜYOR ✅\\033[0m\")\n",
        "        else:\n",
        "            print(f\"\\nSONUÇ: ⚖️ DURUM BELİRSİZ (Eşit oy)\")\n",
        "\n",
        "# --- ÇALIŞTIR ---\n",
        "target_file = \"/content/1 (58).wav\"\n",
        "analyze_audio_file(target_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8aqPq3pyu_a",
        "outputId": "01ee8c3e-ae4a-4ee0-b6fa-6b15456ce69d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANALİZ BAŞLIYOR: /content/1 (58).wav\n",
            "--------------------------------------------------\n",
            "HATA: Dosya bulunamadı! Yolu kontrol et.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# --- AYARLAR ---\n",
        "audio_path = \"/content/1 (58).wav\"  # Ses dosyasının yolu\n",
        "csv_path = \"/content/1 (58).csv\"    # CSV dosyasının yolu\n",
        "TARGET_SAMPLE_RATE = 16000\n",
        "INPUT_LENGTH = 48000 # 3 saniye\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- ÖNCEKİ TAHMİN FONKSİYONU (Hazır dursun) ---\n",
        "def process_and_predict_segment(segment, sr, model):\n",
        "    tensor = torch.from_numpy(segment).float()\n",
        "\n",
        "    # 3 Saniyeye tamamla veya kes\n",
        "    if tensor.shape[0] > INPUT_LENGTH:\n",
        "        center = tensor.shape[0] // 2\n",
        "        start = center - (INPUT_LENGTH // 2)\n",
        "        tensor = tensor[start : start + INPUT_LENGTH]\n",
        "    elif tensor.shape[0] < INPUT_LENGTH:\n",
        "        padding = INPUT_LENGTH - tensor.shape[0]\n",
        "        tensor = F.pad(tensor, (0, padding))\n",
        "\n",
        "    input_tensor = tensor.unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        probs = F.softmax(output, dim=1)\n",
        "        conf, pred = torch.max(probs, 1)\n",
        "\n",
        "    return pred.item(), conf.item()\n",
        "\n",
        "# --- CSV İLE KESİN ANALİZ ---\n",
        "def analyze_with_csv(audio_file, annotation_file):\n",
        "    print(f\"CSV DOĞRULAMALI ANALİZ BAŞLIYOR...\")\n",
        "    print(f\"Ses: {audio_file}\")\n",
        "    print(f\"Veri: {annotation_file}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Dosyaları Yükle\n",
        "    if not os.path.exists(audio_file) or not os.path.exists(annotation_file):\n",
        "        print(\"HATA: Ses veya CSV dosyası bulunamadı!\")\n",
        "        return\n",
        "\n",
        "    y, sr = librosa.load(audio_file, sr=TARGET_SAMPLE_RATE)\n",
        "    df = pd.read_csv(annotation_file)\n",
        "\n",
        "    votes = {\"healthy\": 0, \"asthma\": 0}\n",
        "\n",
        "    print(f\"CSV içinde {len(df)} adet etiketli öksürük bulundu.\\n\")\n",
        "\n",
        "    # Her satırı tek tek işle\n",
        "    for index, row in df.iterrows():\n",
        "        start_time = row['StartTime']\n",
        "        end_time = row['EndTime']\n",
        "\n",
        "        # Saniyeyi sample sayısına çevir (Örn: 1.5 sn * 16000 = 24000. sample)\n",
        "        start_sample = int(start_time * sr)\n",
        "        end_sample = int(end_time * sr)\n",
        "\n",
        "        # Sesi kes\n",
        "        segment = y[start_sample:end_sample]\n",
        "\n",
        "        # Eğer çok kısaysa (CSV hatası vs) atla\n",
        "        if len(segment) < 100:\n",
        "            print(f\"Satır {index+1}: Ses çok kısa, atlanıyor.\")\n",
        "            continue\n",
        "\n",
        "        # Modele Sor\n",
        "        label_idx, confidence = process_and_predict_segment(segment, sr, model)\n",
        "\n",
        "        # Çıktıyı hazırla\n",
        "        label_str = \"SAĞLIKLI\" if label_idx == 0 else \"ASTIM\"\n",
        "        color = \"\\033[92m\" if label_idx == 0 else \"\\033[91m\" # Yeşil/Kırmızı\n",
        "        reset = \"\\033[0m\"\n",
        "\n",
        "        print(f\"Öksürük {index+1} ({start_time}-{end_time}s): {color}{label_str}{reset} (Güven: %{confidence*100:.2f})\")\n",
        "\n",
        "        if label_idx == 0:\n",
        "            votes[\"healthy\"] += 1\n",
        "        else:\n",
        "            votes[\"asthma\"] += 1\n",
        "\n",
        "    # GENEL SONUÇ\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"TOPLAM SONUÇ: {votes['asthma']} Astım vs {votes['healthy']} Sağlıklı\")\n",
        "\n",
        "    if votes[\"asthma\"] > votes[\"healthy\"]:\n",
        "        print(f\"\\nSONUÇ: \\033[91m⚠️ BU HASTA ASTIM (CSV Verisine Göre) ⚠️\\033[0m\")\n",
        "    elif votes[\"healthy\"] > votes[\"asthma\"]:\n",
        "        print(f\"\\nSONUÇ: \\033[92m✅ BU HASTA SAĞLIKLI ⚠️\\033[0m\")\n",
        "    else:\n",
        "        print(f\"\\nSONUÇ: ⚖️ Eşitlik var.\")\n",
        "\n",
        "# Çalıştır\n",
        "analyze_with_csv(audio_path, csv_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLp0Jo3FzzOk",
        "outputId": "c98cf89a-e515-4c65-c811-bdcbce93e237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV DOĞRULAMALI ANALİZ BAŞLIYOR...\n",
            "Ses: /content/1 (58).wav\n",
            "Veri: /content/1 (58).csv\n",
            "------------------------------------------------------------\n",
            "HATA: Ses veya CSV dosyası bulunamadı!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy.signal import butter, lfilter\n",
        "\n",
        "# --- AYARLAR ---\n",
        "TARGET_SAMPLE_RATE = 16000\n",
        "INPUT_LENGTH = 48000 # 3 saniye\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Test edilecek dosya yolu\n",
        "raw_audio_path = \"/content/1 (58).wav\"\n",
        "\n",
        "# --- SİNYAL İŞLEME FONKSİYONLARI ---\n",
        "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    \"\"\"Sesi filtreler: Sadece öksürük frekanslarını bırakır.\"\"\"\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n",
        "\n",
        "def smart_segmentation(y, sr):\n",
        "    \"\"\"\n",
        "    Sesi analiz eder ve öksürük olabilecek aralıkları döndürür.\n",
        "    Gürültüyü değil, patlayıcı sesleri (öksürük) hedefler.\n",
        "    \"\"\"\n",
        "    # 1. Önce sesi temizle (200Hz - 4000Hz arası insan sesi/öksürük aralığıdır)\n",
        "    y_filtered = butter_bandpass_filter(y, 200, 4000, sr, order=4)\n",
        "\n",
        "    # 2. Hassas ayırma (Top_db'yi düşürdükçe hassasiyet artar)\n",
        "    # frame_length: Analiz penceresi. Küçük olması anlık patlamaları yakalar.\n",
        "    intervals = librosa.effects.split(y_filtered, top_db=25, frame_length=1024, hop_length=256)\n",
        "\n",
        "    valid_intervals = []\n",
        "    for start, end in intervals:\n",
        "        duration = (end - start) / sr\n",
        "        # 0.2 saniyeden kısa sesleri (çıtırtı) ve 2 saniyeden uzunları (konuşma olabilir) filtrele\n",
        "        if duration > 0.15 and duration < 2.5:\n",
        "            valid_intervals.append((start, end))\n",
        "\n",
        "    return valid_intervals\n",
        "\n",
        "# --- TAHMİN FONKSİYONU ---\n",
        "def predict_segment(segment, sr, model):\n",
        "    tensor = torch.from_numpy(segment).float()\n",
        "\n",
        "    # 3 Saniye Ayarı\n",
        "    if tensor.shape[0] > INPUT_LENGTH:\n",
        "        center = tensor.shape[0] // 2\n",
        "        start = center - (INPUT_LENGTH // 2)\n",
        "        tensor = tensor[start : start + INPUT_LENGTH]\n",
        "    elif tensor.shape[0] < INPUT_LENGTH:\n",
        "        padding = INPUT_LENGTH - tensor.shape[0]\n",
        "        tensor = F.pad(tensor, (0, padding))\n",
        "\n",
        "    input_tensor = tensor.unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        probs = F.softmax(output, dim=1)\n",
        "        conf, pred = torch.max(probs, 1)\n",
        "    return pred.item(), conf.item()\n",
        "\n",
        "# --- ANA ÇALIŞTIRMA ---\n",
        "def auto_analyze(file_path):\n",
        "    print(f\"OTOMATİK ANALİZ BAŞLIYOR: {os.path.basename(file_path)}\")\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(\"HATA: Dosya bulunamadı.\")\n",
        "        return\n",
        "\n",
        "    # Sesi yükle\n",
        "    y, sr = librosa.load(file_path, sr=TARGET_SAMPLE_RATE)\n",
        "\n",
        "    # Akıllı Segmentasyon Yap\n",
        "    intervals = smart_segmentation(y, sr)\n",
        "\n",
        "    print(f\"Filtreleme sonrası tespit edilen öksürük sayısı: {len(intervals)}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    votes = {\"healthy\": 0, \"asthma\": 0}\n",
        "\n",
        "    for i, (start, end) in enumerate(intervals):\n",
        "        # DİKKAT: Filtrelenmiş sesi değil, ORİJİNAL sesi kesiyoruz\n",
        "        segment = y[start:end]\n",
        "\n",
        "        # Tahmin\n",
        "        label_idx, confidence = predict_segment(segment, sr, model)\n",
        "\n",
        "        label_str = \"SAĞLIKLI\" if label_idx == 0 else \"ASTIM\"\n",
        "        color = \"\\033[92m\" if label_idx == 0 else \"\\033[91m\"\n",
        "        reset = \"\\033[0m\"\n",
        "\n",
        "        # Süre hesapla\n",
        "        dur = (end - start) / sr\n",
        "        time_mark = f\"{start/sr:.1f}s - {end/sr:.1f}s\"\n",
        "\n",
        "        print(f\"[{i+1}] {time_mark} ({dur:.2f}sn) -> {color}{label_str}{reset} (Güven: %{confidence*100:.1f})\")\n",
        "\n",
        "        # Sadece güveni yüksek olanları oylamaya kat\n",
        "        if confidence > 0.60:\n",
        "            if label_idx == 0: votes[\"healthy\"] += 1\n",
        "            else: votes[\"asthma\"] += 1\n",
        "\n",
        "    # SONUÇ\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"OYLAMA SONUCU: {votes['asthma']} Astım - {votes['healthy']} Sağlıklı\")\n",
        "\n",
        "    if votes[\"asthma\"] > votes[\"healthy\"]:\n",
        "        print(f\"GENEL TANI: \\033[91mASTIM BELİRTİLERİ\\033[0m\")\n",
        "    elif votes[\"healthy\"] > votes[\"asthma\"]:\n",
        "        print(f\"GENEL TANI: \\033[92mSAĞLIKLI\\033[0m\")\n",
        "    else:\n",
        "        print(\"GENEL TANI: BELİRSİZ\")\n",
        "\n",
        "# Çalıştır\n",
        "auto_analyze(raw_audio_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxiG_S6q1HcF",
        "outputId": "0987ef66-ff17-4b02-fc89-93717d4e838b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OTOMATİK ANALİZ BAŞLIYOR: 1 (58).wav\n",
            "HATA: Dosya bulunamadı.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test edilecek yeni dosya yolu\n",
        "target_file = \"/content/WhatsApp-Ptt-2025-12-16-at-19.40.47.wav\"\n",
        "\n",
        "# Fonksiyonu çağır (Önceki hücredeki fonksiyon hafızada duruyor, tekrar tanımlamaya gerek yok)\n",
        "if 'auto_analyze' in globals():\n",
        "    auto_analyze(target_file)\n",
        "else:\n",
        "    print(\"HATA: Önceki analiz kodunu (def auto_analyze...) içeren hücreyi çalıştırmamışsın. Lütfen yukarı çıkıp o uzun kodu bir kere çalıştır.\")"
      ],
      "metadata": {
        "id": "dlyQMmpU2POs",
        "outputId": "ebf0ea6d-7493-4d24-8ea7-71133a9e9b59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OTOMATİK ANALİZ BAŞLIYOR: WhatsApp-Ptt-2025-12-16-at-19.40.47.wav\n",
            "Filtreleme sonrası tespit edilen öksürük sayısı: 3\n",
            "--------------------------------------------------\n",
            "[1] 1.0s - 1.3s (0.32sn) -> \u001b[92mSAĞLIKLI\u001b[0m (Güven: %97.4)\n",
            "[2] 1.4s - 1.7s (0.34sn) -> \u001b[92mSAĞLIKLI\u001b[0m (Güven: %89.2)\n",
            "[3] 1.9s - 2.2s (0.30sn) -> \u001b[92mSAĞLIKLI\u001b[0m (Güven: %98.7)\n",
            "--------------------------------------------------\n",
            "OYLAMA SONUCU: 0 Astım - 3 Sağlıklı\n",
            "GENEL TANI: \u001b[92mSAĞLIKLI\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}